{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_load as dl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, ZeroPadding1D, Conv2DTranspose, UpSampling2D, Dense\n",
    "from keras.layers.pooling import MaxPooling2D, MaxPooling1D\n",
    "from keras import optimizers\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\Anaconda3\\lib\\site-packages\\openpyxl\\reader\\worksheet.py:318: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\Patrick\\Datasets\\sr\\SR\"\n",
    "trial_paths = dl.combine_data(path)\n",
    "trials = dl.generate_trial_objects(trial_paths)\n",
    "processed_trials = dl.process_trial_data(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smallest motion sample is 17559\n",
    "# All G samples are 3030 long\n",
    "# Take 2880 points from middle of 3030\n",
    "# 3030 - 2880 = 150\n",
    "#150/2 = 75\n",
    "#Start each (Gs and downsampled motion on point 75) then sample out 2880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synch_streams(G3, G4, motion):\n",
    "    #G2 = G2[75:-75]\n",
    "    G3 = G3[75:-75]\n",
    "    G4 = G4[75:-75]\n",
    "    #G5 = G5[75:-75]\n",
    "    \n",
    "    new_motion = motion[0:-1:6]\n",
    "    new_motion = motion[75:(75+2880)]\n",
    "    \n",
    "    return G3, G4, new_motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segments(G3, G4, motion):\n",
    "    full_run_length = 2880\n",
    "    num_segments = 200\n",
    "    indices = np.random.randint(0,(full_run_length - 112), size=(num_segments))\n",
    "    \n",
    "    images_out = []\n",
    "    motion_out = []\n",
    "    \n",
    "    for i in indices:\n",
    "        #G2_seg = G2[i:(i+112)]/np.amax(G2[i:(i+112)])\n",
    "        G3_seg = G3[i:(i+112)]/np.amax(G3[i:(i+112)])\n",
    "        G4_seg = G4[i:(i+112)]/np.amax(G4[i:(i+112)])\n",
    "        #G5_seg = G5[i:(i+112)]/np.amax(G5[i:(i+112)])\n",
    "        motion_seg = motion[i:(i+112)]/np.amax(motion[i:(i+112)])\n",
    "        \n",
    "        new_image = np.array([[G3_seg],[G4_seg]])\n",
    "        \n",
    "        images_out.append(new_image)\n",
    "        \n",
    "        motion_out.append(np.asarray(motion_seg))\n",
    "        \n",
    "    \n",
    "\n",
    "    return images_out, motion_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(25):\n",
    "    trial = processed_trials[i]\n",
    "    G3_temp, G4_temp, motion_temp = synch_streams(trial.G3_AVG, trial.G4_AVG, trial.Motion_Conv)\n",
    "    images, motion = generate_segments(G3_temp, G4_temp, motion_temp)\n",
    "    \n",
    "    for image in images:\n",
    "        new_image=np.array(image)\n",
    "        new_image.reshape(1,112,2)\n",
    "        X.append(new_image)\n",
    "    \n",
    "    for entry in motion:\n",
    "        y.append(np.array(entry))\n",
    "        \n",
    "X = np.array(X).reshape(5000,1,112,2)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial = processed_trials[5]\n",
    "# G3_temp, G4_temp, motion_temp = synch_streams(trial.G3_AVG, trial.G4_AVG, trial.Motion_Conv)\n",
    "# images, motion = generate_segments(G3_temp, G4_temp, motion_temp)\n",
    "\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for image in images:\n",
    "#     new_image=np.array(image)\n",
    "#     new_image.reshape(1,112,2)\n",
    "#     X.append(new_image)\n",
    "    \n",
    "# for entry in motion:\n",
    "#     y.append(np.array(entry))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 112, 2)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 112)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.expand_dims(X, axis=2)\n",
    "#X = np.expand_dims(X, axis=3)\n",
    "\n",
    "y = np.expand_dims(y, axis=1)\n",
    "y.reshape(5000,1,112)\n",
    "y = np.expand_dims(y, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 112, 2)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1, 112, 1)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(64, (2,2), padding='same', activation='relu', input_shape=(100,1,2)))\n",
    "# #model.add(MaxPooling2D(2))\n",
    "\n",
    "# # model.add(Conv2D(24, (2,2), padding='same', activation='relu'))\n",
    "# # model.add(MaxPooling2D(1,2))\n",
    "\n",
    "# # model.add(Conv2DTranspose(1, (1,1), strides=(1,1), activation='relu'))\n",
    "# # model.add(UpSampling2D(size=(1,2)))\n",
    "\n",
    "# #model.add(Conv2DTranspose(1, (1,1), strides=(1,1), activation='relu'))\n",
    "# #model.add(UpSampling2D(size=(2,1)))\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,112,2), name=\"main_input\")\n",
    "\n",
    "x1 = Conv2D(64, kernel_size=(1,3), padding=\"same\")(inputs)\n",
    "x1 = Conv2D(64, kernel_size=(1,3),padding=\"same\")(x1)\n",
    "x1 = MaxPooling2D(pool_size=(1,2), strides=(2))(x1)\n",
    "\n",
    "x2 = Conv2D(128, kernel_size=(1,3),padding=\"same\")(x1)\n",
    "x2 = Conv2D(128, kernel_size=(1,3),padding=\"same\")(x2)\n",
    "x2 = MaxPooling2D(pool_size=(1,2), strides=(2))(x2)\n",
    "\n",
    "x3 = Conv2D(256, kernel_size=(1,3),padding=\"same\")(x2)\n",
    "x3 = Conv2D(256, kernel_size=(1,3),padding=\"same\")(x3)\n",
    "x3 = MaxPooling2D(pool_size=(1,2), strides=(2))(x3)\n",
    "\n",
    "x4 = Conv2D(512, kernel_size=(1,3),padding=\"same\")(x3)\n",
    "x4 = Conv2D(512, kernel_size=(1,3),padding=\"same\")(x4)\n",
    "x4 = MaxPooling2D(pool_size=(1,2), strides=(2))(x4)\n",
    "\n",
    "x5 = Conv2D(1024, kernel_size=(1,7))(x4)\n",
    "\n",
    "x6 = Dense(1024)(x5)\n",
    "\n",
    "#Presently doing unpooling in hacked way (No filtering for max value, naive upsampling)\n",
    "# https://arxiv.org/pdf/1311.2901v3.pdf\n",
    "# https://stackoverflow.com/questions/44991470/using-tensorflow-layers-in-keras\n",
    "\n",
    "#Need to create custom keras layer to take mask from MaxPooling2D (similar to TF's max_pool_with_argmax)\n",
    "#mask is used in unpooling layer\n",
    "\n",
    "x7 = Conv2DTranspose(512, kernel_size=(1,7), padding='valid')(x6)\n",
    "x7 = UpSampling2D((1,2))(x7)\n",
    "\n",
    "x8 = Conv2DTranspose(512, kernel_size=(1,3), padding='same')(x7)\n",
    "x8 = Conv2DTranspose(512, kernel_size=(1,3), padding='same')(x8)\n",
    "x8 = UpSampling2D((1,2))(x8)\n",
    "\n",
    "x9 = Conv2DTranspose(512, kernel_size=(1,3), padding='same')(x8)\n",
    "x9 = Conv2DTranspose(256, kernel_size=(1,3), padding='same')(x9)\n",
    "x9 = UpSampling2D((1,2))(x9)\n",
    "\n",
    "x10 = Conv2DTranspose(256, kernel_size=(1,3), padding='same')(x9)\n",
    "x10 = Conv2DTranspose(128, kernel_size=(1,3), padding='same')(x10)\n",
    "x10 = UpSampling2D((1,2))(x10)\n",
    "\n",
    "x11 = Conv2DTranspose(128, kernel_size=(1,3), padding='same')(x10)\n",
    "x11 = Conv2DTranspose(64, kernel_size=(1,3), padding='same')(x11)\n",
    "\n",
    "x11 = Conv2DTranspose(2, kernel_size=(1,3), padding='same')(x11)\n",
    "x12 = Conv2DTranspose(1, kernel_size=(1,3), padding='same')(x11)\n",
    "\n",
    "model1 = Model(inputs=inputs, outputs=x12)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.001, clipvalue=0.5)\n",
    "\n",
    "model1.compile(optimizer=sgd, loss = 'mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     print(layer)\n",
    "#     print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1984/5000 [==========>...................] - ETA: 3:33 - loss: 0.0531 - acc: 0.0055"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-313-6d5c62b5a870>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1.fit(X,y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X.npy\",X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
